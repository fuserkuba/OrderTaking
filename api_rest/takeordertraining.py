# -*- coding: utf-8 -*-
"""TakeOrderTraining

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EFITH9h8DQYiqZFuELu3IkVGIUCZOMqQ

LOADING DATA
"""

import pandas as pd
import numpy as np
from IPython.display import display

filename = "https://raw.githubusercontent.com/fuserkuba/OrderTaking/master/training/orders.csv"

df = pd.read_csv(filename, na_values=[""], parse_dates=['created_at'], infer_datetime_format=True)

df.info()

"""## FEATURE ENGINEERING"""

from collections import Counter

display(df.head())
display(df.describe())
print("Dataset: {}".format(df.shape))
print('Dataset classes %s' % Counter(df.taken))

df.total_earning.replace(0,np.nan, inplace=True)
df.to_user_distance.replace(0,np.nan, inplace=True)
df.dropna(inplace=True)

df['month']=df.created_at.dt.month
# Monday is 0 and Sunday is 6
df['weekday']=df.created_at.dt.weekday
df['day']=df.created_at.dt.day
df['hour']=df.created_at.dt.hour
df['working_time']=(df.created_at - pd.to_timedelta(8,unit='H')).dt.hour*60+df.created_at.dt.minute

min_qt=0.01
max_qt=0.99

# filter extreme values
df_ready = df[df.total_earning.gt(df.total_earning.quantile(min_qt))
        & df.total_earning.lt(df.total_earning.quantile(max_qt))
        & df.to_user_distance.gt(df.to_user_distance.quantile(min_qt))
        & df.to_user_distance.lt(df.to_user_distance.quantile(max_qt))
        & df.to_user_elevation.gt(df.to_user_elevation.quantile(min_qt))
        & df.to_user_elevation.lt(df.to_user_elevation.quantile(max_qt))]

print("\nFiltered dataset: {}".format(df_ready.shape))
print('Filtered dataset classes %s' % Counter(df_ready.taken))
display(df_ready.describe())

"""# TRAINING"""

features = ['to_user_distance','to_user_elevation','total_earning','weekday','day','hour','working_time']
target = ['taken']

X = df_ready[features].values
y = df_ready[target].values.ravel()

display(X[:5],X.shape)
display(y[:5],y.shape)

"""## PREPARE TRAIN AND TEST DATA"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)

print("Training set X: {}  y: {}".format(X_train.shape,y_train.shape))
print("Test set X: {}  y: {}".format(X_test.shape,y_test.shape))

print('Training dataset shape %s' % Counter(y_train))
print('Test dataset shape %s' % Counter(y_test))

from imblearn.combine import SMOTEENN # Combine over- and under-sampling using SMOTE and Edited Nearest Neighbours.
from imblearn.over_sampling import SMOTE # Class to perform over-sampling using SMOTE.
from imblearn.under_sampling import RandomUnderSampler # Under-sample the majority class(es) by randomly picking samples with or without replacement.
from imblearn.pipeline import Pipeline


#sme = SMOTEENN(random_state=42)
# define pipeline
over = SMOTE(sampling_strategy=0.2)
under = RandomUnderSampler(sampling_strategy=0.5)
steps = [('o', over), ('u', under)]
pipeline = Pipeline(steps=steps)

# transform the dataset
X_res, y_res = pipeline.fit_resample(X_train, y_train)

print('Resampled dataset shape %s' % Counter(y_train))
print('Resampled dataset shape %s' % Counter(y_res))

X_train = X_res
y_train = y_res

from sklearn.preprocessing import StandardScaler
#
from sklearn.pipeline import Pipeline
#
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
# import xgboost as xgb

#classifier = KNeighborsClassifier(n_neighbors=3)
classifier = RandomForestClassifier(criterion='gini',max_features='auto',n_jobs=-1)
#classifier = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)

steps = [('scaler', StandardScaler()),
         ('clf', classifier)]

pipeline = Pipeline(steps)

# parameters = {'clf__n_neighbors': (3,5,10)}
parameters = {
'clf__max_depth': [100],
'clf__max_features': [2],
'clf__min_samples_leaf': [3],
'clf__min_samples_split': [8],
'clf__n_estimators': [400],
'clf__class_weight': ['balanced_subsample']
}

display(parameters)

"""## CROSS VALIDATION"""

from sklearn.model_selection import GridSearchCV
from pprint import pprint
from time import time

grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='roc_auc')

print("Performing grid search...")
print("pipeline:", [name for name, _ in pipeline.steps])
t0 = time()
grid_search.fit(X_train, y_train)
print("Done in %0.3fs" % (time() - t0))
print()
print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameters.keys()):
  print("\t%s: %r" % (param_name, best_parameters[param_name]))

model=grid_search.best_estimator_

print("\nBEST MODEL :")
display(model)

print("BEST SCORE : {}".format(grid_search.best_score_))

"""## TEST MODEL"""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score

y_pred = model.predict(X_test)

print("confusion matrix:")
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print("ROC_AUC: %0.2f" % roc_auc_score(y_test, y_pred))
print("Accuracy: %0.2f" % accuracy_score(y_test, y_pred))

"""# SAVE MODEL"""

import joblib

filename='model.joblib'

joblib.dump(model, filename)