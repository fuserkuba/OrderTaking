# -*- coding: utf-8 -*-
"""TakeOrderTraining

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EFITH9h8DQYiqZFuELu3IkVGIUCZOMqQ

LOADING DATA
"""

import pandas as pd
import numpy as np
from IPython.display import display

filename = "https://raw.githubusercontent.com/fuserkuba/OrderTaking/master/training/orders.csv"

df = pd.read_csv(filename, na_values=[""], parse_dates=['created_at'], infer_datetime_format=True)

df.info()

"""## FEATURE ENGINEERING"""

from collections import Counter

display(df.head())
display(df.describe())
print("Dataset: {}".format(df.shape))
print('Dataset classes %s' % Counter(df.taken))

df.total_earning.replace(0, np.nan, inplace=True)
df.to_user_distance.replace(0, np.nan, inplace=True)
df.dropna(inplace=True)

df['month'] = df.created_at.dt.month
# Monday is 0 and Sunday is 6
df['weekday'] = df.created_at.dt.weekday.apply(str)
df['day'] = df.created_at.dt.day
df['hour'] = df.created_at.dt.hour
df['working_time'] = (df.created_at - pd.to_timedelta(8, unit='H')).dt.hour * 60 + df.created_at.dt.minute

min_qt = 0.01
max_qt = 0.99

# filter extreme values on mayority class
df_ready = df[df.taken.eq(0) |
              (df.total_earning.gt(df.total_earning.quantile(min_qt))
               & df.total_earning.lt(df.total_earning.quantile(max_qt))
               & df.to_user_distance.gt(df.to_user_distance.quantile(min_qt))
               & df.to_user_distance.lt(df.to_user_distance.quantile(max_qt))
               & df.to_user_elevation.gt(df.to_user_elevation.quantile(min_qt))
               & df.to_user_elevation.lt(df.to_user_elevation.quantile(max_qt)))]

print("\nFiltered dataset: {}".format(df_ready.shape))
print('Filtered dataset classes %s' % Counter(df_ready.taken))
display(df_ready.describe())

"""# TRAINING"""

features = ['to_user_distance', 'to_user_elevation', 'total_earning', 'weekday', 'working_time']
target = ['taken']

X = df_ready[features].values
y = df_ready[target].values.ravel()

display(X[:5], X.shape)
display(y[:5], y.shape)

"""## PREPARE TRAIN AND TEST DATA"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)

print("Training set X: {}  y: {}".format(X_train.shape, y_train.shape))
print("Test set X: {}  y: {}".format(X_test.shape, y_test.shape))

print('Training dataset shape %s' % Counter(y_train))
print('Test dataset shape %s' % Counter(y_test))

"""### RESAMPLING"""

from imblearn.over_sampling import SMOTE  # Class to perform over-sampling using SMOTE.
from imblearn.under_sampling import RandomUnderSampler # Under-sample the majority class(es) by randomly picking samples with or without replacement.
from imblearn.pipeline import Pipeline

# define pipeline
over = SMOTE(sampling_strategy=0.6)
under = RandomUnderSampler(sampling_strategy=0.8)
steps = [('o', over), ('u', under)]
pipeline = Pipeline(steps=steps)

# transform the dataset
X_res, y_res = pipeline.fit_resample(X_train, y_train)

print('Trained dataset shape %s' % Counter(y_train))
print('Resampled dataset shape %s' % Counter(y_res))

X_train = X_res
y_train = y_res

from sklearn.preprocessing import StandardScaler

from sklearn.pipeline import Pipeline

from sklearn.ensemble import RandomForestClassifier

classifier = RandomForestClassifier(n_jobs=-1)

steps = [('scaler', StandardScaler()),
         ('clf', classifier)]

pipeline = Pipeline(steps)

parameters = {
    'clf__criterion': ['entropy'],
    'clf__max_depth': [200],
    'clf__max_features': [2],
    'clf__min_samples_leaf': [2],
    'clf__min_samples_split': [4],
    'clf__n_estimators': [300],
    'clf__class_weight': ['balanced_subsample']
}

display(parameters)

"""## CROSS VALIDATION"""

from sklearn.model_selection import GridSearchCV
from time import time

grid_search = GridSearchCV(pipeline, parameters, n_jobs=2, verbose=1, scoring='balanced_accuracy', cv=2)

print("Performing grid search...")
print("pipeline:", [name for name, _ in pipeline.steps])
t0 = time()
grid_search.fit(X_train, y_train)
print("Done in %0.3fs" % (time() - t0))
print()
print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameters.keys()):
    print("\t%s: %r" % (param_name, best_parameters[param_name]))

model = grid_search.best_estimator_

print("\nBEST MODEL :")
display(model)

print("BEST SCORE : {}".format(grid_search.best_score_))

"""## TEST MODEL"""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import accuracy_score

y_pred = model.predict(X_test)

print("confusion matrix:")
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print("Recall score: %0.2f" % recall_score(y_test, y_pred, average='weighted'))
print("F1 score: %0.2f" % f1_score(y_test, y_pred, average='weighted'))
# The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.
print("Balanced accuracy score: %0.2f" % balanced_accuracy_score(y_test, y_pred))
print("Accuracy: %0.2f" % accuracy_score(y_test, y_pred))

"""# SAVE MODEL"""

import joblib

filename = 'model.joblib'

joblib.dump(model, filename)
